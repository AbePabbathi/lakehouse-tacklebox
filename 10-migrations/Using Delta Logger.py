# Databricks notebook source
# MAGIC %md
# MAGIC
# MAGIC ## Delta Logger - How to use
# MAGIC
# MAGIC Purpose: This notebook utilizes the delta logger library to automatically and easiy log general pipeline information all in one place for any data pipeline. 
# MAGIC
# MAGIC All logger tables have a standard default schema DDL: 
# MAGIC
# MAGIC CREATE TABLE IF NOT EXISTS delta_logger (
# MAGIC   run_id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   process_name STRING NOT NULL,
# MAGIC   status STRING NOT NULL, -- RUNNING, FAIL, SUCCESS, STALE
# MAGIC   start_timestamp TIMESTAMP NOT NULL,
# MAGIC   end_timestamp TIMESTAMP,
# MAGIC   run_metadata STRING
# MAGIC )
# MAGIC USING DELTA 
# MAGIC PARTITIONED BY (process_name);
# MAGIC
# MAGIC ## Initialize 
# MAGIC delta_logger = DeltaLogger(logger_table="main.iot_dashboard.pipeline_logs", 
# MAGIC                             process_name="iot_pipeline", 
# MAGIC                             logger_location=None)
# MAGIC
# MAGIC  - <b>logger_table</b> is the logging table you want to store and reference. You can create and manage as many logger tables as you would like. If you initilize a DeltaLogger and that table does not exist, it will create it for you. 
# MAGIC - <b> process_name</b> OPTIONAL - Users can log events/runs and pass the process_name into each event, or they can simply define it at the session level this way. This will default to using the process_name passed in here for the whole session. It can be overridden anytime. 
# MAGIC - <b> logger_location </b> OPTIONAL - default = None. This is an override for specifying a specific object storage location for where the user wants the table to live. If not provided, it will be a managed table by default (recommended).
# MAGIC
# MAGIC ## Methods: 
# MAGIC
# MAGIC For most methods: --  if process_name not provided, will use session. If cannot find process_name, will error. 
# MAGIC
# MAGIC - <b> create_logger() </b> -- creates a logger table if not exists. This also optimizes the table since it is used in initlialization. 
# MAGIC - <b> drop_logger() </b> -- drops the logger table attached to the session
# MAGIC - <b> truncate_logger() </b> -- clears an existing logger table
# MAGIC - <b> start_run(process_name: Optional, msg: Optional) </b>
# MAGIC - <b> fail_run(process_name: Optional, msg: Optional) </b>
# MAGIC - <b> complete_run(process_name: Optional, msg: Optional) </b>
# MAGIC - <b> get_last_successful_run_id(proces_name: Optional)</b> -- If no previous successful run, return -1
# MAGIC - <b> get_last_successful_run_timestamp(process_name: Optional)</b> -- If no previous successful run for the process, defaults to "1900-01-01 00:00:00"
# MAGIC - <b> get_last_run_id(process_name: Optional)</b> -- Get last run id regardless of status, if none return -1
# MAGIC - <b> get_last_run_timestamp(process_name: Optional)</b> -- Get last run timestamp , If no previous run for the process, defaults to "1900-01-01 00:00:00"
# MAGIC - <b> get_last_failed_run_id(process_name: Optional) </b>
# MAGIC - <b> get_last_failed_run_timestamp(prcoess_name: Optional) </b>
# MAGIC - <b> clean_zombie_runs(process_name: Optional) </b> -- Will mark any runs without and end timestamp in the running state to "STALE" and give them an end timestamp. This ONLY happens when a new run is created and the runs are < the max existing RUNNING run id
# MAGIC - <b> optimize_log(process_name:Optional, zorderCols=["end_timestamp", "start_timestamp", "run_id"]) </b> -- Optimizes the underlying log table for a particular process name a ZORDERs by input col list
# MAGIC - <b> INTERNAL: _update_run_id(run_id, process_name:Optional, start_time=None, end_time=None, status=None, run_metadata=None)
# MAGIC
# MAGIC ### Limitations / Considerations
# MAGIC 1. Currently supports 1 concurrent run per process_name for a given delta table. If you want to run concurrent pipelines, you need to create separate process names for them. This is meant to be a simple run and logging tracking solution for EDW pipelines. 
# MAGIC
# MAGIC 2. User can pass in the fully qualified table name, use the spark session defaults, or pass in catalog and database overrides to the parameters. Pick one. 
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## Design Patterns
# MAGIC
# MAGIC 1. Use for Basic error handling, tracking of runs of various processes
# MAGIC 2. Use for watermarking loading patterns. i.e. Creating a new run automatically pulls the most recent previous successful run and provide a "watermark" variable you can utilize for incremental loading. Use delta_logger.get_last_succes

# COMMAND ----------

from helperfunctions.deltalogger import DeltaLogger

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC CREATE DATABASE IF NOT EXISTS main.iot_dashboard_logger;
# MAGIC USE CATALOG main;
# MAGIC USE DATABASE iot_dashboard_logger;

# COMMAND ----------

delta_logger = DeltaLogger(logger_table_name="main.iot_dashboard_logger.delta_logger", process_name='iot_dashboard_pipeline')

# COMMAND ----------

delta_logger.get_most_recent_success_run_start_time()

# COMMAND ----------

delta_logger.create_run(metadata={"data_quality_stuff": "oh dear"})

# COMMAND ----------

print(delta_logger.active_run_id)
print(delta_logger.active_run_end_ts)
print(delta_logger.active_run_start_ts)
print(delta_logger.active_run_status)
print(delta_logger.active_run_metadata)

# COMMAND ----------

# DBTITLE 1,Complete and Fail Active Runs
delta_logger.complete_run()
#delta_logger.fail_run()

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC SELECT * FROM main.iot_dashboard_logger.delta_logger
